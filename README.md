# SUMMERINTERNSHIP2024_22it122

## 13th May, 2024
# Summer Internship 2024

## Introduction
This repository documents my experiences, learning, and projects during my summer internship as a Software Devloper, starting from 13th May, 2024, and concluding on 21th June, 2024.

## Internship Overview
My summer internship at NAVPAD Infotech began on 13th May, 2024, and will conclude on 21th June, 2024. As a Software Devloper, I am responsible for working with frontend designing, implementing user interface, testing module and debugging method to alter the running test software module also design database and complete process of software requirements gathering techniques and analysis in test system
## Responsibilities

## Weekly Progress

### Week of 13th May to 17th May, 2024
-Monday: Attended an introduction meeting from 9:00 to 11:00 AM to get acquainted with the team, followed by a detailed project briefing by Nirmit Sir from 12:00 to 3:00 PM. Began creating the backend infrastructure for the project from 4:00 to 6:00 PM.

-Tuesday: Participated in an intern meeting from 9:00 to 11:00 AM to discuss initial thoughts and roles. Spent 12:00 to 3:00 PM creating essential database tables (Student, Assignment, Faculty), and dedicated 4:00 to 6:00 PM to simplifying and refactoring the initial codebase.

-Wednesday: Completed the development of the Login and Register pages from 9:00 to 11:00 AM, added basic Read functionality between 12:00 and 3:00 PM, and worked on integrating other CRUD functionalities from 4:00 to 6:00 PM, enhancing the application's data management capabilities.

-Thursday: Created a GitHub repository and added team members, also downloaded GitHub Desktop from 9:00 to 11:00 AM. Attended another intern meeting from 12:00 to 3:00 PM to review progress and set goals, and updated the code uploaded on GitHub from 4:00 to 6:00 PM to ensure version control and collaboration.

-Friday: Focused on creating additional necessary tables in the database from 9:00 to 11:00 AM, simplified and optimized the codebase from 12:00 to 3:00 PM, and had a progress review discussion with Sir from 4:00 to 6:00 PM, addressing any issues and planning the next steps.

### Week 2: May 20th - May 24th
- This week, I focused on developing a Power BI dashboard to visualize sales data, which was my first major project. I began by familiarizing myself with the Power BI interface, including its various tools and - -features, to create effective and interactive visualizations. My initial task was to connect to the company's sales database, where I performed data extraction, transformation, and loading (ETL) processes to ensure that the data was clean, consistent, and ready for analysis.

- Throughout the week, I explored different visualization options such as bar charts, line graphs, and pie charts to represent sales trends, comparisons, and performance metrics accurately. I learned how to use filters, slicers, and other interactive elements to allow users to drill down into the data for more detailed insights. Additionally, I created calculated columns and measures using DAX (Data Analysis Expressions) to enhance the dashboard's analytical capabilities.

- To ensure that the dashboard met the needs of the end-users, I collaborated closely with team members and stakeholders to understand their specific requirements and the key performance indicators (KPIs) they needed to monitor. This collaboration helped me tailor the dashboard to provide relevant and actionable insights.

- I also spent time learning how to integrate Power BI with other data sources and tools, such as Excel and SQL Server, which broadened my understanding of the data ecosystem within the company. This integration knowledge is crucial for creating comprehensive and connected data solutions.

By the end of the week, I had developed a functional prototype of the sales dashboard. This prototype included various visualizations that effectively communicated sales performance and trends. I presented the prototype to my team for feedback, which included suggestions for improvements and additional features. This iterative process will help refine the dashboard and ensure it meets all user requirements.

![Sales Dashboard](images/sales_dashboard.png)

### Week 3: May 27th - May 31st
- This week, I focused on learning about web scraping and practicing on weather data. Web scraping is the process of extracting data from websites, which can then be used for analysis and reporting. I started by familiarizing myself with the basic concepts of web scraping, including understanding HTML structure, identifying the elements to extract, and using libraries to automate the extraction process.

- I learned to use popular Python libraries such as BeautifulSoup and requests to perform web scraping tasks. BeautifulSoup is used to parse HTML and XML documents, allowing easy navigation and data extraction, while requests is used to send HTTP requests to websites to retrieve their content.

- For my practice project, I chose to scrape weather data from a weather forecasting website. This involved writing Python scripts to send requests to the website, parse the HTML content, and extract relevant information such as temperature, humidity, and weather conditions. I then cleaned and structured the data to make it suitable for analysis.

- Throughout the week, I encountered and resolved several challenges, such as handling dynamic content and dealing with websites that block scraping attempts. I also learned about ethical considerations and legal issues related to web scraping, ensuring that my practices complied with the website's terms of service.

- By the end of the week, I successfully scraped and processed weather data, which I stored in a CSV file for further analysis. This hands-on experience with web scraping not only improved my Python programming skills but also provided valuable insights into data collection from web sources, a crucial skill for data analysts.

### Week 4: June 3rd - June 7th
- This week, I focused on Exploratory Data Analysis (EDA), an essential step in understanding the structure, patterns, and relationships within a dataset. For the first two days, I dedicated my time to learning the fundamentals of EDA. I explored various techniques such as data visualization, summary statistics, and correlation analysis to uncover insights and identify data quality issues. Using Python libraries like pandas, matplotlib, and seaborn, I practiced these techniques on sample datasets to solidify my understanding.

- After grasping the basics of EDA, I began working on my internship project, which involved predicting house prices in Delhi. This project required me to apply my newly acquired EDA skills to analyze the dataset thoroughly. I started by loading the house price data and performing initial data cleaning, such as handling missing values and removing duplicates. I then generated summary statistics and visualizations to understand the distribution of house prices and the relationships between different features, such as location, size, and amenities.

- To gain deeper insights, I conducted correlation analysis to identify which features had the most significant impact on house prices. I created various plots, including scatter plots, box plots, and histograms, to visualize these relationships. These visualizations helped me identify trends and outliers, guiding me in selecting the most relevant features for the prediction model.

- Throughout the week, I documented my findings and prepared the data for the modeling phase, which would follow in the subsequent weeks. This hands-on experience with EDA not only enhanced my analytical skills but also provided a strong foundation for building accurate and reliable predictive models.

### Week 5: June 10th - June 14th
- This week, I delved into the basics of Machine Learning (ML), which is essential for my final internship project on house price prediction. The first few days were dedicated to understanding the fundamental concepts of ML, including supervised and unsupervised learning, training and testing datasets, and model evaluation metrics. I utilized various online resources and tutorials to grasp these concepts and their practical applications.

- Following my introduction to ML, I focused on learning two key algorithms: Random Forest and Linear Regression. Random Forest is an ensemble learning method that operates by constructing multiple decision trees and merging their results to improve accuracy and prevent overfitting. Linear Regression, on the other hand, is a straightforward yet powerful algorithm for predicting a target variable based on one or more predictor variables.

- To solidify my understanding, I practiced implementing these algorithms using Python libraries such as scikit-learn. I started with Linear Regression, where I learned how to fit a model to data, interpret coefficients, and evaluate model performance using metrics like Mean Squared Error (MSE) and R-squared. Next, I moved on to Random Forest, experimenting with hyperparameter tuning and feature importance to enhance model accuracy.

- By the end of the week, I had a solid grasp of both algorithms and was ready to apply them to my internship project. I started training the Delhi home price prediction model using the dataset I had prepared earlier. This involved splitting the data into training and testing sets, fitting the models, and evaluating their performance. This hands-on experience with ML models provided me with practical skills that are crucial for building and refining predictive models.

- This foundational knowledge in ML will be crucial as I continue to work on improving the house price prediction model in the coming weeks.

### Week 6: June 17th - June 21st
- This week, I began exploring new concepts in Machine Learning (ML) that are necessary for my new final internship project: Loan Approval Prediction. This project will span the next three weeks, and my initial focus was on understanding the specific requirements and challenges associated with predicting loan approvals.

- To start, I familiarized myself with the dataset, which includes various features such as applicant income, credit history, loan amount, and other relevant factors. I performed data cleaning and preprocessing tasks to ensure the data was in a suitable format for analysis. This involved handling missing values, encoding categorical variables, and normalizing numerical features.

- Next, I researched different ML algorithms that are commonly used for classification tasks, such as Logistic Regression, Decision Trees, and Support Vector Machines (SVM). I explored their theoretical foundations and practical applications, comparing their strengths and weaknesses in the context of loan approval prediction.

- Throughout the week, I also focused on feature engineering, a crucial step in improving model performance. I generated new features that could potentially enhance the predictive power of the model. For instance, I created derived features such as debt-to-income ratio and loan-to-value ratio, which are important indicators in the loan approval process.

- By the end of the week, I had a solid understanding of the dataset and the ML techniques required for the Loan Approval Prediction project. This foundational knowledge will guide my work in the upcoming weeks as I build, evaluate, and refine the predictive models.

### Week 7: June 24th - June 28th
- This week, I focused on understanding the data provided by my external guide for the Loan Approval Prediction project, which will be the main focus of my final internship project over the next three weeks.

- The initial step involved thoroughly examining the dataset to understand its structure, content, and relevance to the loan approval prediction problem. The dataset included a variety of features such as applicant details (e.g., income, credit score), loan specifics (e.g., amount requested, loan term), and application status (approved or denied).

- I began by exploring the data to gain a high-level overview of its characteristics. This involved loading the data into a Pandas DataFrame and performing preliminary tasks such as checking for missing values, data types, and basic statistics for each feature. I used methods like `info()`, `describe()`, and `head()` to get a sense of the data's completeness and distributions.

- I then delved deeper into understanding the features and their relevance to the loan approval decision. This included analyzing feature distributions, identifying correlations between features, and understanding the impact of various features on loan approval outcomes. I used visualizations like histograms, box plots, and scatter plots to explore feature distributions and relationships.

- One significant task was to map out the data’s business context. I reached out to my external guide to gain insights into the decision-making process for loan approvals, including key criteria used by banks and financial institutions. This understanding helped me frame the problem more effectively and identify which features were likely to be most predictive of loan approval.

- I also began researching relevant machine learning techniques and methodologies for classification tasks, specifically for loan approval predictions. I reviewed different algorithms, such as Logistic Regression, Decision Trees, Random Forest, and Gradient Boosting Machines, to evaluate which methods might be most suitable for the project based on the data's characteristics and the project's requirements.

- In addition, I explored various techniques for feature selection and engineering, which are crucial for improving model performance. I examined methods like Recursive Feature Elimination (RFE), feature importance from models, and correlation-based feature selection to identify the most relevant features for the prediction model.

- By the end of the week, I had a comprehensive understanding of the dataset and the problem domain. I documented my findings and prepared a detailed plan for the next phases of the project, which include data preprocessing, model building, and evaluation. This foundational work set the stage for developing a robust and effective loan approval prediction model.

- This week’s efforts provided me with the necessary background knowledge and prepared me for the upcoming tasks in the loan approval prediction project.

### Week 8: July 1st - July 5th
## Basics of Machine Learning

### What is Machine Learning?
Machine Learning (ML) is a field of artificial intelligence that focuses on developing algorithms and statistical models that enable computers to learn from data and make predictions or decisions without being explicitly programmed. ML encompasses various methods and techniques to analyze data, identify patterns, and make informed predictions.

### Types of Machine Learning
1. **Supervised Learning:** Models are trained on labeled data with known outcomes. Common algorithms include Linear Regression for continuous outcomes and Logistic Regression for binary classification.

2. **Unsupervised Learning:** Models work with unlabeled data to find patterns or group similar data points. Techniques include K-Means Clustering for grouping data and Principal Component Analysis (PCA) for dimensionality reduction.

3. **Semi-Supervised Learning:** Combines a small amount of labeled data with a large amount of unlabeled data. It’s useful for situations where labeling data is costly or time-consuming.

4. **Reinforcement Learning:** Models learn by interacting with an environment and receiving rewards or penalties. Examples include Q-Learning and Deep Q-Networks.

### Basic Algorithms
- **Linear Regression:** Predicts a continuous value.
- **Logistic Regression:** Classifies binary outcomes.
- **Decision Trees:** Makes decisions based on feature values.
- **Random Forest:** Uses an ensemble of decision trees.

### Model Evaluation Metrics
- **Accuracy:** Proportion of correct predictions.
- **Precision:** True positives divided by the sum of true and false positives.
- **Recall:** True positives divided by the sum of true positives and false negatives.

### Feature Engineering and Hyperparameter Tuning
- **Feature Engineering:** Creating or modifying features to improve model performance.
- **Hyperparameter Tuning:** Adjusting model parameters to enhance performance using techniques like Grid Search and Random Search.

### Overfitting and Underfitting
- **Overfitting:** Model performs well on training data but poorly on new data.
- **Underfitting:** Model is too simple to capture the data’s patterns.

## Skills Learned
- Proficiency in Power BI for creating and maintaining interactive dashboards.
- Advanced skills in Tableau for data visualization and storytelling.
- Data analysis using Python, including processes and predictive analytics.
- Strong understanding of data cleaning, preparation, and quality assurance.
- Effective communication and presentation skills for sharing insights and findings.

## Conclusion
Overall, this internship has been a valuable learning experience. I gained practical skills in data analysis, visualization, and automation using Power BI, Tableau, and Python. The projects I worked on have provided me with insights into real-world data challenges and solutions. I am grateful for the opportunity to contribute to the team and grow as a Data Analyst.


---
